{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import re\n",
    "\n",
    "\n",
    "class LimeExplainer():\n",
    "    def __init__(self, model_name, nli=False):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to('cuda')\n",
    "        self.baseline_token = '[MASK]'\n",
    "        self.device = 'cuda'\n",
    "        self.nli = nli\n",
    "\n",
    "    def predictor(self, texts):\n",
    "        texts = [t.replace('[[MASK]]', '[SEP]') for t in texts]\n",
    "        if self.nli:\n",
    "            sent1 = []\n",
    "            sent2 = []\n",
    "            for t in texts:\n",
    "                s1, s2 = t.split('[SEP]')\n",
    "                sent1.append(s1)\n",
    "                sent2.append(s2)\n",
    "\n",
    "        all_probs = []\n",
    "        for i in range(math.ceil(len(texts) / self.bs)):\n",
    "            batch_idx = slice(i * self.bs, (i + 1) * self.bs)\n",
    "            if self.nli:\n",
    "                encoded = self.tokenizer(sent1[batch_idx],\n",
    "                                        text_pair=sent2[batch_idx],\n",
    "                                        return_tensors=\"pt\",\n",
    "                                        padding=True).to(self.device)\n",
    "            else:\n",
    "                encoded = self.tokenizer(texts[batch_idx],\n",
    "                                        return_tensors=\"pt\",\n",
    "                                        padding=True).to(self.device)\n",
    "            outputs = self.model(**encoded)\n",
    "            probs = torch.softmax(outputs.logits, -1).detach().cpu()\n",
    "            all_probs.append(probs)\n",
    "        probs = torch.cat(all_probs, 0).numpy()\n",
    "        return probs\n",
    "\n",
    "    def explain(self, str_to_predict, batch_size=32, mask_n=5000, output_indices=None):\n",
    "        self.bs = batch_size\n",
    "\n",
    "        explainer = LimeTextExplainer(class_names=['contradiction', 'entailment', 'neutral'],\n",
    "                                      bow=False,\n",
    "                                      mask_string=self.baseline_token)\n",
    "        exp = explainer.explain_instance(str_to_predict,\n",
    "                                         self.predictor,\n",
    "                                         top_labels=2,\n",
    "                                         num_features=20,\n",
    "                                         num_samples=mask_n)\n",
    "        pred = int(exp.predict_proba.argmax())\n",
    "        tokens = re.split(r'\\W+', str_to_predict)\n",
    "        tokens = [tok if tok != 'SEP' else '[SEP]' for tok in tokens]\n",
    "        if output_indices is None:\n",
    "            output_indices = pred\n",
    "        explanations = {(k,): v for k, v in exp.as_map()[output_indices]}\n",
    "        return exp, explanations, tokens, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime = LimeExplainer('textattack/bert-base-uncased-snli', nli=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp, explanation, tokens, pred = lime.explain(\"A soccer game with multiple males playing. [SEP] Some men are playing a sport.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(5,): 0.21130668071150888,\n",
       " (13,): 0.16711675887591976,\n",
       " (1,): 0.14968719404536854,\n",
       " (8,): -0.06588197184775399,\n",
       " (2,): 0.06479727033633015,\n",
       " (9,): -0.05534492112108581,\n",
       " (6,): 0.05176177471145916,\n",
       " (11,): -0.04457048769407898,\n",
       " (12,): 0.028630411137390396,\n",
       " (0,): -0.02518382129868726,\n",
       " (4,): -0.019109104346571495,\n",
       " (3,): 0.018757777425323866,\n",
       " (10,): -0.010720534914372328,\n",
       " (7,): -0.0046395738476809915}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime = LimeExplainer('textattack/bert-base-uncased-SST-2', nli=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp, explanation, tokens, pred = lime.explain(\"This film doesn't care about cleverness, wit or any other kind of intelligent humor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.save_to_file('exp_nli.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c2746b4c817b2e7822e65c51ec19822c8b0676bc7037025bbef03eb15b20d6a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('synch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
